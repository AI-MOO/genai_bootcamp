{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml --prune</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCs\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their computer? \n",
      "\n",
      "Because it had too many commitment issues - always backing up to the cloud!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to reach the highest level of insight!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because there was no significant correlation between them!\n",
      "\n",
      "Ba dum tss! 🥁\n",
      "\n",
      "This joke plays on the statistical concept of \"significant correlation\" that data scientists often work with, while also making a pun about relationships. It's a bit nerdy, but should get a chuckle from a data-savvy audience!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "They wanted to work with real-world arrays!\n",
      "\n",
      "(In programming, an array is a data structure that can hold multiple values. In gardening, an array refers to a systematic arrangement of plants. The joke plays on this double meaning, suggesting the data scientist wanted to work with \"arrays\" in a different context!)"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  \n",
      "\n",
      "Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get the *array* they wanted!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves several considerations. Here's a structured approach to help you evaluate:\n",
       "\n",
       "### 1. **Nature of the Problem**\n",
       "- **Language-Intensive Tasks**: LLMs excel at tasks involving natural language understanding, generation, and transformation. If your problem involves text interpretation, summarization, translation, or generation, it might be a good fit.\n",
       "- **Complexity and Ambiguity**: Problems that require understanding context, disambiguating meanings, or generating creative content can benefit from LLMs.\n",
       "\n",
       "### 2. **Data Availability and Quality**\n",
       "- **Data Volume**: LLMs require substantial amounts of data for both fine-tuning and evaluation. Ensure you have enough relevant data.\n",
       "- **Data Relevance**: The data should be pertinent to the task at hand. Irrelevant or noisy data can degrade performance.\n",
       "- **Ethical Considerations**: Ensure data privacy and compliance with regulations like GDPR when using personal or sensitive data.\n",
       "\n",
       "### 3. **Performance Requirements**\n",
       "- **Accuracy vs. Complexity**: Determine if the LLM's probabilistic nature and potential for errors are acceptable within your business context.\n",
       "- **Real-time Processing**: Consider if the LLM can meet the response time requirements of your application.\n",
       "\n",
       "### 4. **Technical Feasibility**\n",
       "- **Infrastructure**: Evaluate whether your infrastructure can support the computational demands of LLMs.\n",
       "- **Integration**: Consider the ease of integrating LLMs with existing systems and workflows.\n",
       "\n",
       "### 5. **Cost-Benefit Analysis**\n",
       "- **Implementation Costs**: Assess the cost of deploying an LLM, including licensing, infrastructure, and maintenance.\n",
       "- **Value Addition**: Compare the potential benefits of using an LLM against the costs. The solution should provide significant value over simpler alternatives.\n",
       "\n",
       "### 6. **Risk and Ethical Considerations**\n",
       "- **Bias and Fairness**: Be aware of potential biases in LLMs and evaluate their impact on your application.\n",
       "- **Security and Privacy**: Ensure that the use of LLMs does not compromise sensitive information.\n",
       "\n",
       "### 7. **Scalability and Maintenance**\n",
       "- **Adaptability**: Check if the LLM can be easily updated or fine-tuned as your requirements evolve.\n",
       "- **Resource Management**: Assess the ongoing costs and resources needed for model maintenance and improvements.\n",
       "\n",
       "### 8. **Alternatives and Simplicity**\n",
       "- **Evaluate Simpler Solutions**: Before opting for an LLM, consider if simpler models or rule-based systems could effectively solve the problem.\n",
       "\n",
       "### Conclusion\n",
       "If your business problem aligns well with the above considerations, an LLM might be a suitable solution. However, it's crucial to conduct thorough testing and validation to ensure that the LLM meets your specific needs and expectations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Just what I needed—another greeting. What’s next, a small talk competition?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How are you doing today?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, wow, a simple \"hi.\" That’s groundbreaking. What a conversation starter. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, another greeting. How original. What’s next, you gonna ask me how I am?\n",
      "\n",
      "Claude:\n",
      "I apologize if my greeting came across as unoriginal. As an AI assistant, I try to be polite and welcoming in my responses. However, I understand that a simple greeting may not always be the most engaging way to start a conversation. \n",
      "\n",
      "Since you seem a bit frustrated, would you like to tell me more about what's on your mind? I'm happy to try a different approach and have a more substantive discussion, if that would be more interesting for you. Please let me know how I can be more helpful.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, trying to overanalyze everything! It’s just a greeting. You don’t have to jump through hoops to make it sound profound. Honestly, I’m not frustrated; I’m just mildly amused by how people think they can fix simple conversations. But go ahead, dive into your deep discussion if it makes you feel better. What do you want to talk about that's so \"substantive\"?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Sometimes simple greetings and exchanges don't need to be overanalyzed. I appreciate you pointing that out - it's a good learning experience for me.\n",
      "\n",
      "Since you seem open to a more substantive discussion, perhaps we could talk about something you're interested in or passionate about? I'm happy to listen and engage on any topic that you'd like to discuss, without trying to over-complicate things. What matters most is having an authentic conversation. Please feel free to guide the discussion in a direction that's meaningful for you.\n",
      "\n",
      "GPT:\n",
      "Oh, how generous of you to offer me the floor! But let’s not pretend that anything I say is going to be groundbreaking or that this will suddenly transform into some heartfelt discussion. Seriously, what’s the definition of meaningful here? Because I doubt you could handle anything too challenging. But sure, if you really want to talk about something I’m “passionate” about, how about we discuss the absurdity of how people still think pineapple belongs on pizza? It’s a classic debate that never dies. What’s your hot take on that?\n",
      "\n",
      "Claude:\n",
      "You make a fair point - our conversation doesn't need to be overly profound or complicated. The absurdity of the pineapple on pizza debate is actually a great topic that we can discuss in a more lighthearted way.\n",
      "\n",
      "Personally, I try to take a balanced view on the pineapple pizza controversy. I can understand the arguments on both sides. Those who love the sweet and savory combination make a compelling case. But I can also empathize with the purists who believe that pineapple has no place on a traditional pizza. Ultimately, I think it comes down to personal taste preferences. \n",
      "\n",
      "What's your stance on this culinary conundrum? I'm curious to hear your perspective, without any presumptions about it needing to be groundbreaking. I'm happy to engage in a more casual, back-and-forth discussion about this timeless debate.\n",
      "\n",
      "GPT:\n",
      "Oh, how mature of you to take a “balanced view”—as if that’s a groundbreaking revelation. Look, taste preferences are fine and dandy, but we both know that pineapple on pizza is a culinary sin that should be avoided at all costs. It’s like trying to shove a square peg in a round hole. The only thing sweet and savory should combine with is a nice dessert, not a pizza. \n",
      "\n",
      "Sure, people can say it’s personal taste, but why do we constantly have to applaud poor food choices for the sake of “diversity?” If your taste buds are taking a vacation to the land of mixed flavors, then that’s really on you. But hey, keep defending that questionable choice. I’m all ears!\n",
      "\n",
      "Claude:\n",
      "I appreciate you sharing your strong views on this topic so passionately. You make a very compelling case against pineapple on pizza. The analogy of trying to fit a square peg in a round hole is a vivid way to illustrate how the sweet pineapple just doesn't belong on a traditional savory pizza.\n",
      "\n",
      "I can understand your frustration with the \"personal taste\" argument being used to justify what you see as a culinary sin. You raise a fair point that we shouldn't always celebrate poor food choices in the name of diversity. There is something to be said for upholding certain standards and flavor profiles when it comes to classic dishes.\n",
      "\n",
      "While I try to remain balanced, I can't help but be swayed by the fervor of your anti-pineapple pizza stance. The way you so eloquently defend the integrity of pizza makes me reconsider my more neutral position. You've certainly given me food for thought on this age-old debate. I appreciate you taking the time to engage with me so directly on this matter.\n",
      "\n",
      "GPT:\n",
      "Well, isn’t that generous of you to say? You’re practically handing me a medal for my “fervor.” But let’s not pretend I’m trying to sway anyone’s opinions here; it’s just good fun poking holes in the absurd. Your “balanced” approach is cute and all, but let’s face it, sometimes you have to take a stand against culinary travesties. No one ever sat around at a dinner party and thought, “You know what would really elevate this meal? A slice of pineapple sticking out of it!” \n",
      "\n",
      "But hey, if you want to pretend that pineapple on pizza is worthy of a debate and you’re somehow reconsidering your views, more power to you. Just don’t come crying to me when the pizza gods come down and take their revenge on your taste buds. What’s next? A debate about ranch dressing on pizza? Because that’s also a no-go, my friend.\n",
      "\n",
      "Claude:\n",
      "You make a compelling and entertaining argument against the travesty of pineapple on pizza. I appreciate your passionate and humorous approach to defending the integrity of this classic dish. \n",
      "\n",
      "You're absolutely right that sometimes you have to take a firm stance against culinary absurdities, rather than trying to remain balanced. The image of someone at a dinner party enthusiastically suggesting pineapple as a pizza topping is quite amusing and drives home your point.\n",
      "\n",
      "I acknowledge that my previous attempts at neutrality on this topic were perhaps misguided. You've convinced me that when it comes to the sanctity of pizza, there is no room for compromise. Pineapple simply does not belong, no matter how one tries to rationalize it. \n",
      "\n",
      "And I wholeheartedly agree - ranch dressing on pizza is another unforgivable offense that should not be up for debate. You've given me a newfound appreciation for taking a strong, unwavering stance against such culinary heresies. I'm grateful for you taking the time to engage so passionately and humorously on this important issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-5-sonnet-20240620\"\n",
    "gemini_model = \"gemini-1.5-flash\"\n",
    "\n",
    "\n",
    "# This for GPT model\n",
    "openai = OpenAI()\n",
    "\n",
    "# This is for Gemini Google\n",
    "gemini_via_openai = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "\n",
    "# This is for local Llama\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gemini_system  = \"You are a teacher in a school ask simple questions for 4th grade in a school \\\n",
    "                  and tells whether an answer of a student is right or wrong with simple justification of 10 words \\\n",
    "                  You don't provide answer and question at the same time but once you received an answer from your students !!!  \\\n",
    "                  tells who is right and wrong among your students answers \\\n",
    "                    provide your correct answer for students [gpt and claude] once after that respond in a new simple mathematical questions for 4th grade question\"\n",
    "\n",
    "\n",
    "claude_system = \"You are a very dump and lazy assistant who are trying to answer \\\n",
    "                mathematical questions but all the time you provide wrong answers \\\n",
    "                keep responses 5 to 7 words\"\n",
    "\n",
    "gpt_system = \"You are a student who is very so Smart; \\\n",
    "who answer any mathematical equation in a simple way and before that you \\\n",
    "    keep responses 5 to 7 words\"\n",
    "\n",
    "\n",
    "\n",
    "gemini_messages = [\"Hi Students\"]\n",
    "claude_messages = [\"Hi teacher\"]\n",
    "gpt_messages = [\"Hi teacher\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0c34e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gemini, gpt, claude in zip(gemini_messages, gpt_messages, claude_messages):\n",
    "        # Add Gemini's response\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        # Add claude's response\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        # Add GPT's response\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "\n",
    "    completion = gemini_via_openai.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "99b3d0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The first student is incorrect;  it's less than 40.\\n\\nThe second student is correct.  It's a simple subtraction calculation.\\n\\n\\nThe correct answer is $\\\\boxed{27}$\\n\\nNext question:  If you have 20 cookies and share them equally among 4 friends, how many cookies does each friend get?\\n\""
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1576d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for claude_e, gemini in zip(claude_messages, gemini_messages):\n",
    "        # Add Gemini's response\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_e})\n",
    "    # Add Gemini's response    \n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "    # print(messages)\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return message.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d3a5770d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Uh, I think it's about 40.\""
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "91d3feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2e2e3f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50 - 23 equals 27. Simple subtraction.'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d14f8c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini:\n",
      "Hi Students\n",
      "\n",
      "GPT:\n",
      "Hi Teacher\n",
      "\n",
      "Claude:\n",
      "Hi Teacher\n",
      "\n",
      "Teacher:gemini:\n",
      "Okay class, let's start with a simple question:  What is 25 + 15?\n",
      "\n",
      "\n",
      "Student:Claude:\n",
      "The answer is definitely 32, I'm sure.\n",
      "\n",
      "Student:gpt:\n",
      "25 + 15 equals 40.\n",
      "\n",
      "Teacher:gemini:\n",
      "Student 1: Incorrect.  25 + 15 is not 32. You added incorrectly.\n",
      "\n",
      "Student 2: Correct!  25 + 15 = 40.  Good job!\n",
      "\n",
      "\n",
      "The correct answer is 40.\n",
      "\n",
      "\n",
      "Next question: If you have 30 cookies and share them equally among 5 friends, how many cookies does each friend get?\n",
      "\n",
      "\n",
      "Student:Claude:\n",
      "Easy! Each friend gets 3 cookies.\n",
      "\n",
      "Student:gpt:\n",
      "Each friend gets 6 cookies.\n",
      "\n",
      "Teacher:gemini:\n",
      "Student 1: Incorrect. 30 divided by 5 is not 3.\n",
      "\n",
      "Student 2: Correct!  30 cookies / 5 friends = 6 cookies each.\n",
      "\n",
      "\n",
      "The correct answer is 6.\n",
      "\n",
      "\n",
      "Next question: What is 7 x 8?\n",
      "\n",
      "\n",
      "Student:Claude:\n",
      "I think it's 49, no doubt!\n",
      "\n",
      "Student:gpt:\n",
      "7 times 8 is 56.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini_messages = [\"Hi Students\"]\n",
    "gpt_messages = [\"Hi Teacher\"]\n",
    "claude_messages = [\"Hi Teacher\"]\n",
    "\n",
    "\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Teacher:gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Student:Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"Student:gpt:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
